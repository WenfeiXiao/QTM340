{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this problem set, we'll do both unsupervised and supervised learning on text.\n",
        "\n",
        "Similar to PS1, you're free to execute the notebook in your personal environment, but I would strongly recommend using Google Colab. You can upload this notebook to Google colab by following the steps below.\n",
        "\n",
        "1. Open [colab.research.google.com](colab.research.google.com)\n",
        "2. Click on the upload tab\n",
        "3. Upload the `.ipynb` file by choosing the right file from your local disk\n",
        "\n",
        "\n",
        "**Submission instructions**\n",
        "\n",
        "1. When you're ready to submit, you'll save the notebook as QTM340-PS2-Firstname-Lastname.ipynb; for example, if your name is Harry Potter, save the file as `QTM340-PS2-Harry-Potter.ipynb`. This can be done in Google colab by editing the filename and then following File --> Download --> .ipynb\n",
        "\n",
        "2. Upload this file on canvas."
      ],
      "metadata": {
        "id": "OVeHdHM5H2d6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this problem set, you'll learn to:\n",
        "\n",
        "(a) Use `gensim` to find topics\n",
        "\n",
        "(b) Use scikit-learn to train multiple classifiers\n",
        "\n",
        "(c) Calculate CI and test hypothesis"
      ],
      "metadata": {
        "id": "_VMyq91SItbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do all of this on a collection of movie plot summaries that are extracted from Wikipedia courtesy of this [paper from Bamman et. al.](http://www.cs.cmu.edu/~dbamman/pubs/pdf/bamman+oconnor+smith.acl13.pdf)."
      ],
      "metadata": {
        "id": "x2IUQqayQ7mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\n",
        "!tar -xzvf MovieSummaries.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U9z7P71YU92",
        "outputId": "3fb8949d-1db7-4e89-eea6-af86a7a6040e"
      },
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-16 07:32:59--  http://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 48002242 (46M) [application/x-gzip]\n",
            "Saving to: ‘MovieSummaries.tar.gz.1’\n",
            "\n",
            "MovieSummaries.tar. 100%[===================>]  45.78M  3.29MB/s    in 14s     \n",
            "\n",
            "2023-10-16 07:33:13 (3.17 MB/s) - ‘MovieSummaries.tar.gz.1’ saved [48002242/48002242]\n",
            "\n",
            "MovieSummaries/\n",
            "MovieSummaries/tvtropes.clusters.txt\n",
            "MovieSummaries/name.clusters.txt\n",
            "MovieSummaries/plot_summaries.txt\n",
            "MovieSummaries/README.txt\n",
            "MovieSummaries/movie.metadata.tsv\n",
            "MovieSummaries/character.metadata.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll find a `MovieSummaries` directory which contains many files. We're interested in the `README.txt`, `plot_summaries.txt`, and `movie.metadata.txt`.\n",
        "\n",
        "Let's print the `README.txt` file."
      ],
      "metadata": {
        "id": "yR092SSDYR3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat MovieSummaries/README.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BNDAziuZC8N",
        "outputId": "9e16243c-83e7-487b-d564-f7b07b0d4319"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This README describes data in the CMU Movie Summary Corpus, a collection of 42,306 movie plot summaries and metadata at both the movie level (including box office revenues, genre and date of release) and character level (including gender and estimated age).  This data supports work in the following paper:\n",
            "\n",
            "David Bamman, Brendan O'Connor and Noah Smith, \"Learning Latent Personas of Film Characters,\" in: Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL 2013), Sofia, Bulgaria, August 2013.\n",
            "\n",
            "All data is released under a Creative Commons Attribution-ShareAlike License. For questions or comments, please contact David Bamman (dbamman@cs.cmu.edu).\n",
            "\n",
            "###\n",
            "#\n",
            "# DATA\n",
            "#\n",
            "###\n",
            "\n",
            "1. plot_summaries.txt.gz [29 M] \n",
            "\n",
            "Plot summaries of 42,306 movies extracted from the November 2, 2012 dump of English-language Wikipedia.  Each line contains the Wikipedia movie ID (which indexes into movie.metadata.tsv) followed by the summary.\n",
            "\n",
            "\n",
            "2. corenlp_plot_summaries.tar.gz [628 M, separate download]\n",
            "\n",
            "The plot summaries from above, run through the Stanford CoreNLP pipeline (tagging, parsing, NER and coref). Each filename begins with the Wikipedia movie ID (which indexes into movie.metadata.tsv).\n",
            "\n",
            "\n",
            "###\n",
            "#\n",
            "# METADATA\n",
            "#\n",
            "###\n",
            "\n",
            "3. movie.metadata.tsv.gz [3.4 M]\n",
            "\n",
            "\n",
            "Metadata for 81,741 movies, extracted from the Noverber 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
            "\n",
            "1. Wikipedia movie ID\n",
            "2. Freebase movie ID\n",
            "3. Movie name\n",
            "4. Movie release date\n",
            "5. Movie box office revenue\n",
            "6. Movie runtime\n",
            "7. Movie languages (Freebase ID:name tuples)\n",
            "8. Movie countries (Freebase ID:name tuples)\n",
            "9. Movie genres (Freebase ID:name tuples)\n",
            "\n",
            "\n",
            "\n",
            "4. character.metadata.tsv.gz [14 M]\n",
            "\n",
            "Metadata for 450,669 characters aligned to the movies above, extracted from the Noverber 4, 2012 dump of Freebase.  Tab-separated; columns:\n",
            "\n",
            "1. Wikipedia movie ID\n",
            "2. Freebase movie ID\n",
            "3. Movie release date\n",
            "4. Character name\n",
            "5. Actor date of birth\n",
            "6. Actor gender\n",
            "7. Actor height (in meters)\n",
            "8. Actor ethnicity (Freebase ID)\n",
            "9. Actor name\n",
            "10. Actor age at movie release\n",
            "11. Freebase character/actor map ID\n",
            "12. Freebase character ID\n",
            "13. Freebase actor ID\n",
            "\n",
            "\n",
            "##\n",
            "#\n",
            "# TEST DATA\n",
            "#\n",
            "##\n",
            "\n",
            "tvtropes.clusters.txt\n",
            "\n",
            "72 character types drawn from tvtropes.com, along with 501 instances of those types.  The ID field indexes into the Freebase character/actor map ID in character.metadata.tsv.\n",
            "\n",
            "name.clusters.txt\n",
            "\n",
            "\n",
            "970 unique character names used in at least two different movies, along with 2,666 instances of those types.  The ID field indexes into the Freebase character/actor map ID in character.metadata.tsv.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset directory contains wikipedia plot summaries for 42306 movies. There is also metadata for 81741 movies. For this assignment, we're interested in the genre of a movie, its wiki id, its release date, and its plot summary"
      ],
      "metadata": {
        "id": "nXHKzW_yS47C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "1hcAoXZOTpFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's read the entire dataset as a pandas dataframe."
      ],
      "metadata": {
        "id": "7N7RRXvvBgtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the plot summaries into a dataframe\n",
        "df = pd.read_csv ('MovieSummaries/plot_summaries.txt', sep='\\t', names=['wiki_id', 'summary'])\n",
        "\n",
        "# Map the wikiid to a number and vice versa\n",
        "wikiid2rownum = {id: i for i, id in enumerate (df.wiki_id.values)}\n",
        "rownum2wikiid = {i: id for i, id in enumerate (df.wiki_id.values)}\n",
        "\n",
        "# Read the metadata about the movies into a dataframe\n",
        "metadata = pd.read_csv (\"MovieSummaries/movie.metadata.tsv\", sep='\\t', names=[\"wiki_id\",\n",
        "                                                                              \"freebase_id\",\n",
        "                                                                              \"name\",\n",
        "                                                                              \"release_date\",\n",
        "                                                                              \"revenue\",\n",
        "                                                                              \"runtime\",\n",
        "                                                                              \"languages\",\n",
        "                                                                              \"countries\",\n",
        "                                                                              \"genres\"])"
      ],
      "metadata": {
        "id": "y0LaFqLjSS8z"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Topic modeling [2 points]\n",
        "\n",
        "We'll first try to do some exploratory analysis based on topic modeling. Particularly, we're interested in finding\n",
        "\n",
        "- the topics in this collection\n",
        "- the correlation of the topics\n",
        "- the prevalence of topics over time"
      ],
      "metadata": {
        "id": "V1xt_UmiT8Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do so, first we'll use the `nltk` library to tokenize the plot summaries. We could have also used `spacy` but it takes too much time."
      ],
      "metadata": {
        "id": "uQgkc0rA9OqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll lower case the summaries, remove all the stop words, and only consider the alphabetic characters. The following code takes 2-3 minutes to run on google colab"
      ],
      "metadata": {
        "id": "zEN0Vy3D_qKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_summaries (summary):\n",
        "  stop = set(stopwords.words ('english'))\n",
        "  tokens = nltk.word_tokenize(summary)\n",
        "  tokens = [token.lower () for token in tokens if token.isalpha() and token not in stop]\n",
        "  return tokens\n",
        "\n",
        "df[\"tokens\"] = df[\"summary\"].progress_apply (lambda x: tokenize_summaries (x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXM-ItxUkLzA",
        "outputId": "00d2343f-e9d1-4107-82c4-e187a52096a8"
      },
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42303/42303 [02:23<00:00, 293.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a dictionary for the corpus. We can do this by calling gensim's `Dictionary` object and passing the tokenized corpus. We'll also apply some light filters to trim the vocabulary to do topic modeling."
      ],
      "metadata": {
        "id": "VeowT4Wnl2rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a dictionary of words from the corpus\n",
        "dictionary = gensim.corpora.Dictionary(df['tokens'])\n",
        "print (f\"Before filtering: {len (dictionary)}\")\n",
        "\n",
        "# Filter the dictionary to meet frequency thresholds\n",
        "dictionary.filter_extremes(no_below=10,\n",
        "                           no_above=0.5,\n",
        "                           keep_n=10000)\n",
        "print (f\"After filtering: {len (dictionary)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok7vE1C4kyHy",
        "outputId": "df3bbc06-c1d2-43e0-c3ff-1b5c08b84b31"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before filtering: 134369\n",
            "After filtering: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By creating a dictionary in gensim, you map every token to an id, allowing you to convert a stream of tokens to a stream of ids. You can access these ids using `token2id` property that gets set. To run the LDA topic model, we'll have to transform the documents"
      ],
      "metadata": {
        "id": "gorq43BquCQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The map of wordids back the words\n",
        "id2token = {id: token for token, id in dictionary.token2id.items()}\n",
        "\n",
        "# Construction of the corpus in the format that gensim expects\n",
        "corpus = [dictionary.doc2bow(doc) for doc in df['tokens']]"
      ],
      "metadata": {
        "id": "KLUNrhYmleTD"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!** Run LDA model for the above corpus to get 10 topics. [0.5 points]\n",
        "\n",
        "You'll want to use `LdaMulticore` model and set the number of passes or sweeps over the data to 5 and number of iterations to 50.\n",
        "\n",
        "**Note:** This may take a few minutes."
      ],
      "metadata": {
        "id": "skXDvdw1tpyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = None\n",
        "# Your code below\n"
      ],
      "metadata": {
        "id": "zA0_ucWBbI-S"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the code below to run, your topic model should be in variable named lda\n",
        "\n",
        "topics = lda.show_topics(num_topics=10, num_words=10, log=True, formatted=False)\n",
        "\n",
        "# We'll print the top words associated with each topic\n",
        "for topic_num, topic_words in topics:\n",
        "  topic_rep = \" \".join ([f\"{w}({p:.4f})\" for w,p in topic_words])\n",
        "  print (f\"Topic {topic_num}: {topic_rep}\")"
      ],
      "metadata": {
        "id": "F75lolaW1Rfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sanity check**: I found one topic which seemed to be about war and army, etc"
      ],
      "metadata": {
        "id": "C9RFQxG1aM6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Your turn!** Now calculate the following distributions as numpy matrices[0.5 points]\n",
        "\n",
        "(a) For every document, get the mixture of topics. We can do this by calling the `get_document_topics` of the lda model method on the entire corpus.\n",
        "\n",
        "(b) For every topic, get the distribution over words. We can do this by calling the `get_topic_terms` of the lda model on the entire vocabulary.\n",
        "\n",
        "You'll implement two functions below to calculate these.\n",
        "\n"
      ],
      "metadata": {
        "id": "wU8_ryjVn--K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_mixture (corpus, lda_model):\n",
        "  \"\"\" Get mixture of topics from the corpus.\n",
        "\n",
        "  :params:\n",
        "  :corpus (list): The transformed corpus where every document is a list\n",
        "  :lda_model (gensim.models.ldamulticore.LdaMulticore): The gensim model\n",
        "\n",
        "  :returns:\n",
        "  topic_mixture (np.array): The corpus is represented as a numpy matrix, where\n",
        "                            each row is a document and the columns correspond\n",
        "                            to topics.\n",
        "\n",
        "                            Size = number of documents x number of topics\n",
        "\n",
        "  \"\"\"\n",
        "  topic_mixture = None\n",
        "  # Your code here\n",
        "\n",
        "  return topic_mixture\n",
        "\n",
        "def get_word_dist (lda_model):\n",
        "  \"\"\" Get the probability distribution of words for all the topics.\n",
        "\n",
        "  :params:\n",
        "  :lda_model (gensim.models.ldamulticore.LdaMulticore): The gensim model\n",
        "\n",
        "  :returns:\n",
        "  word_dist (np.array): The topics are represented as a numpy matrix, where\n",
        "                        each row is a topic and the columns correspond to the\n",
        "                        words.\n",
        "\n",
        "                        Size = number of topics x number of words\n",
        "  \"\"\"\n",
        "  word_dist = None\n",
        "  # Your code here\n",
        "\n",
        "  return word_dist"
      ],
      "metadata": {
        "id": "eT2j3IKyonOI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's call these functions to get the distributions."
      ],
      "metadata": {
        "id": "KNkDal_jEISR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_mix = get_topic_mixture (corpus, lda)\n",
        "word_dist = get_word_dist (lda)"
      ],
      "metadata": {
        "id": "zbSGnRQR-Dzm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In fact, we'll just add all the topic probabilities to the dataframe"
      ],
      "metadata": {
        "id": "YMBgJC-KMVbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (lda.num_topics):\n",
        "  df[f'topic_{i}'] = topic_mix[:,i]"
      ],
      "metadata": {
        "id": "MoO7FOnkMa9v"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that we have added additional columns to the dataframe to have the distribution of topics for every movie."
      ],
      "metadata": {
        "id": "pU6OHQ1jE75v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "paPxUHeYMdcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!** Now let's find the correlation between the topics. We can do this by calculating the cosine distance between each pair of topics using the code from problem set 1 or you can use [this scikit-learn method](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). Notice that every topic is a vectory of probabilities over words.\n",
        "\n",
        "Give 3 topic pairs with the strongest correlation and 3 topic pairs with the weakest correlation? [0.5 points]"
      ],
      "metadata": {
        "id": "lI5higYgAlqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here to calculate the cosine distances\n"
      ],
      "metadata": {
        "id": "xfVIJd6soTi9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(dist, cmap='Greys', interpolation='nearest')"
      ],
      "metadata": {
        "id": "P9J8RmBQpxBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!** Find the distribution of topics over time. Following are the steps that you want to follow, which you'll implement in each cell below [0.5 points]"
      ],
      "metadata": {
        "id": "c_y5EdYd1Nu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the release date for each movie in the `release_date` column of the metadata frame.  We want to first filter the dataframe to exclude any missing values (NaN)"
      ],
      "metadata": {
        "id": "xhzwMslaf-vT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 filter the metadata (Your code below)\n"
      ],
      "metadata": {
        "id": "88NN_M8MgvAc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Next, you'll create two new columns for the year of release and decade of the release. Note that the release date is either the year directly or the full date string."
      ],
      "metadata": {
        "id": "vWWXJF4-JZDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_year (pub_date):\n",
        "  \"\"\" Extract the year from the publication date string\n",
        "  :params:\n",
        "  pub_date (str): The publication date as a string\n",
        "\n",
        "  :returns:\n",
        "  year (int): The release year of the movie\n",
        "  \"\"\"\n",
        "  pass\n",
        "  # Your code below\n",
        "\n",
        "\n",
        "def get_decade (year):\n",
        "  \"\"\" Convert the year into the decade i.e 2017 --> 2010; 1992 -->1990\n",
        "\n",
        "  :params:\n",
        "  year (int): The year of release\n",
        "\n",
        "  :returns:\n",
        "  decade (int): The decade of the release\n",
        "  \"\"\"\n",
        "  pass\n",
        "  # Your code below\n",
        "\n",
        "\n",
        "# Step 2 add the two columns\n",
        "new_metadata[\"year\"] = new_metadata[\"release_date\"].apply (lambda x: extract_year (x))\n",
        "new_metadata[\"decade\"] = new_metadata[\"year\"].apply (lambda x: get_decade (x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev5zKGON2Lq6",
        "outputId": "a602e964-7462-4198-d219-149abb36e448"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-118f78c621ab>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  new_metadata[\"year\"] = new_metadata[\"release_date\"].apply (lambda x: extract_year (x))\n",
            "<ipython-input-17-118f78c621ab>:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  new_metadata[\"decade\"] = new_metadata[\"year\"].apply (lambda x: get_decade (x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now join the dataframes that contain the text and the topics with the metadata by matching the field `wiki_id` from both the dataframes."
      ],
      "metadata": {
        "id": "sKzvSd4HkRbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3 Join the two dataframes on the common column (Your code below)\n"
      ],
      "metadata": {
        "id": "0BgSOo8DiwCo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Finally, calculate the average probability per decade from 1900 to 2010. You may want to use pandas `groupby` function from the pandas library"
      ],
      "metadata": {
        "id": "Pre5ANFhlJwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decades = [1900,\n",
        "           1910,\n",
        "           1920,\n",
        "           1930,\n",
        "           1940,\n",
        "           1950,\n",
        "           1960,\n",
        "           1970,\n",
        "           1980,\n",
        "           1990,\n",
        "           2000,\n",
        "           2010]\n",
        "\n",
        "# Step 4: calculate the average probability by grouping (Your code below)\n"
      ],
      "metadata": {
        "id": "ckS-sA2PlUjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_decade_plot.plot(x=\"decade\", y=[f\"topic_{i}\" for i in range (10)], alpha=0.5)"
      ],
      "metadata": {
        "id": "gOZv3-u7m-t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sanity check!** I found the \"war\" topic to peak in the 1910's then fall during the great depression and rise again in the 50's after which it has remained quite steady. I'm not sure if it matches any hypothesis but it's interesting nonetheless!"
      ],
      "metadata": {
        "id": "UBTI7sFLGYiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prediction [4 points]"
      ],
      "metadata": {
        "id": "7nXhcObjqN-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll develop multiple regression models to predict the box office revenue of a movie. We'll start by first filtering the dataframe to contain only those movies which don't have any missing data."
      ],
      "metadata": {
        "id": "D3fknStRqRGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_df = overall_df.dropna() # remove all the rows that contain any missing values\n",
        "regression_df = regression_df.query (\"decade in @decades\")"
      ],
      "metadata": {
        "id": "hGgnhJM1b5T3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have the `regression_df` dataframe, we'll create another column called `tokenized_text` as follows"
      ],
      "metadata": {
        "id": "lrPj56u52tAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_df[\"tokenized_text\"] = regression_df[\"tokens\"].apply (lambda x:\" \".join (x))"
      ],
      "metadata": {
        "id": "WpI1gF7L28c-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what the regression dataframe looks like"
      ],
      "metadata": {
        "id": "48muj8MTQfJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_df.head (5)"
      ],
      "metadata": {
        "id": "yMb_3aCXQjKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above simply joins the individual tokens from the plot summary in any movie into a string in which the tokens are separated by a whitespace"
      ],
      "metadata": {
        "id": "bQy3R7HvHXch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll split the movies into a train and a test set as follows"
      ],
      "metadata": {
        "id": "PgOPrcemvuoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_wikiids, test_wikiids = train_test_split(regression_df.wiki_id,\n",
        "                                               test_size=0.2,\n",
        "                                               random_state=42)"
      ],
      "metadata": {
        "id": "1zADQ2VMvEQ0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create an array for our dependent variable in both the train and test sets"
      ],
      "metadata": {
        "id": "_ok7u2XdHt6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = regression_df.query ('wiki_id in @train_wikiids').revenue.values\n",
        "y_test = regression_df.query ('wiki_id in @test_wikiids').revenue.values"
      ],
      "metadata": {
        "id": "-40ZWZHx1tae"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you'll need to use a tfidf vectorizer for some of the models which can be done in the following"
      ],
      "metadata": {
        "id": "fFlbqAlX34FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer (sublinear_tf=True,\n",
        "                              max_features=500,\n",
        "                              max_df=0.5,\n",
        "                              min_df=5)"
      ],
      "metadata": {
        "id": "v3_y6b_837SI"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll develop nested regression models from the following feature sets:\n",
        "\n",
        "1. **Length of the movie (M)**: This feature is a scalar value that can be obtained directly by accessing the `runtime` field in the dataframe.\n",
        "\n",
        "2. **Genres (G)**: Every movie is associated with a dictionary of genres, which can be accessed using the `genres` column in the dataframe. The key in this dictionary is a freebase id and the value is a genre name in plain english. To use genres as features, we'll need to convert the plain english names to a vector. Fortunately, we know how to do this tranformation -- by treating the different genres for a movie as a bag-of-genres vector. Thus, if there are K unique genres in total, the genres feature vector is of size K and the dimensions are either 0 or 1 indicating the presence or absence of a genre.\n",
        "\n",
        "3. **Plot summaries (S)**: Every movie also contains a summary. We'll featurize the summary by creating a tfidf vector for each summary. The vocabulary size should be capped at 500.\n",
        "\n",
        "4. **Topics (T)**: Every movie can be represented by a topic mixtures vector which we obtained in the previous section.\n",
        "\n",
        "**Your turn!** You'll fill the following table for the different nested models with the root mean squared error between the predicted revenue for each model and the actual revenue. State the model that performs best [2 points]\n",
        "\n",
        "\n",
        "|Models|RMSE|\n",
        "|------|----|\n",
        "|M||\n",
        "|G||\n",
        "|S||\n",
        "|T||\n",
        "|M+G||\n",
        "|G+S||\n",
        "|S+T||\n",
        "|M+G+S||\n",
        "|G+S+T||\n",
        "|M+G+S+T||\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "- Our regression model will use the features to predict the log of the revenue although the RMSE will be calculated by comparing the actual revenues.\n",
        "\n",
        "- [RMSE](https://statisticsbyjim.com/regression/root-mean-square-error-rmse/) is a measure to evaluate the performance of a model by comparing the model predictions to the ground truth value. You can calculate RMSE by using [this function from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) setting the `squared` argument to `False`. A smaller RMSE value suggest a more accurate model\n",
        "\n",
        "- The addition of models here means that the combined model has all the features from the individual models. For example, an M+D model means the features from the M model and the D model are concatenated.\n",
        "\n",
        "- You may want to use [numpy's hstack](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html) method to concatenate the feature vectors\n",
        "\n",
        "- You may also want to use the tfidf vectorizer we created earlier to get the tfidf features for each document [See this](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "sL0uyBmDjGqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build the above models, we'll implement the following functions.\n",
        "\n",
        "\n",
        "**Your turn!** First, a function to train and test a regression model for the different features. By passing different feature matrices, we can reuse this function to train and test all our regression models [0.5 points]"
      ],
      "metadata": {
        "id": "OJNIkiv8Kadu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test (X_train, y_train, X_test, y_test):\n",
        "  \"\"\" Trains a linear regression model, makes predictions with\n",
        "  the trained model, and calculates the root mean squared error\n",
        "  between the predicted values and the true values\n",
        "\n",
        "  NOTE: Instead of regressing against the revenue, it might be better to predict\n",
        "  the log of the revenue\n",
        "\n",
        "  :params:\n",
        "  :X_train (np.array): The predictors in the regression problem for the training data\n",
        "  :y_train (np.array): The dependent variable in the regression problem for the training data\n",
        "  :X_test (np.array): The predictors in the regression problem for the testing data\n",
        "  :y_test (np.array): The dependent variable in the regression problem for the testing data\n",
        "\n",
        "  :returns: the following triple as a tuple\n",
        "  :lr (LinearRegression): The linear regression model obtained from sklearn\n",
        "  :yhat (np.array): The array of predictions from the regression model on test data\n",
        "  :err (float): The RMSE error between predicted values and test data outputs\n",
        "  \"\"\"\n",
        "\n",
        "  lr, yhat, err = None, None, None\n",
        "  # Your code below\n",
        "\n",
        "  return lr, np.exp(yhat), err"
      ],
      "metadata": {
        "id": "Eap76kpMIFhs"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!** Next, we'll write a function that transforms the genres to their features [0.5 points]\n",
        "\n",
        "**Hint!** Use sklearn's countvectorizer to extract the features"
      ],
      "metadata": {
        "id": "KP3hhG2mLaDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_genre_features (genres):\n",
        "  \"\"\" Convert the plain english names of the genres from a dictionary\n",
        "      to bag of words vector.\n",
        "\n",
        "  :params:\n",
        "  :genres (np.array): Every item in the array is a string representation of a\n",
        "                      dictionary, which lists all the genres for the movie.\n",
        "                      e.g. [{\"/m/0lsxr\": \"Crime\"},\n",
        "                            {\"/m/07s9rl0\": \"sci-fi\", \"/m/01jfsb\":\"thriller},\n",
        "                            ...]\n",
        "\n",
        "  :returns: pair of objects as tuple\n",
        "  genre_mat (np.array): A binary matrix of size = num_movies X num_genres;\n",
        "                        1 represents whether the movie corresponding to the row\n",
        "                        is of the genre corresponding to the column\n",
        "\n",
        "\n",
        "  genre2index (dict): A dictionary that maps the genre name to column number\n",
        "\n",
        "\n",
        "  Note: To convert string representations of a dict to a dict, we can use\n",
        "  the eval function\n",
        "  \"\"\"\n",
        "  pass\n",
        "  # Your code below"
      ],
      "metadata": {
        "id": "qLbOTMKmLmec"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!** We'll also write a utility function that helps preserve the train and test indices from the overall sequence of wikiids [0.5 points]\n",
        "\n",
        "\n",
        "For example:\n",
        "Suppose the sequence of wikiids was originally [1234, 5632, 756, 8354, 18792]. When we split into a train and test set, the train sequence turned out to be [8354, 1234, 756] and the test sequence as [18792, 5632]. Our function should give us an array of train indices as [3, 0, 2] and test indices as [4, 1]."
      ],
      "metadata": {
        "id": "7DzafLFNWJZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_test_indices (all_ids, train_ids, test_ids):\n",
        "  \"\"\" Find the indices of elements from train_ids and test_ids in the all_ids.\n",
        "\n",
        "  e.g. all_ids = [1234, 5632, 756, 8354, 18792],\n",
        "       train_ids = [8354, 1234, 756],\n",
        "       test_ids = [18792, 5632]\n",
        "\n",
        "       The output should be two lists:\n",
        "       train_indices = [3,0,2],\n",
        "       test_indices = [4,1]\n",
        "\n",
        "  :params:\n",
        "  :all_ids (np.array): All the wikipedia ids\n",
        "  :train_ids (np.array): All the wikipedia ids part of the training set\n",
        "  :test_ids (np.array): All the wikipedia ids part of the test set\n",
        "\n",
        "  :returns: the following pair as a tuple\n",
        "  :train_indices (np.array): The train indices for all the train ids\n",
        "  :test_indices (np.array): The test indices for all the test ids\n",
        "  \"\"\"\n",
        "\n",
        "  train_indices, test_indices = [], []\n",
        "  # Your code below\n",
        "\n",
        "  return train_indices, test_indices"
      ],
      "metadata": {
        "id": "tGuZ1M82W9qY"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let me show you how you can calculate the RMSE with the above functions for one such model -- predicting the revenue by the runtime of the movie. Long movies may not produce great box-office revenue but this predictor is unlikely to be great because there isn't much variability in the runtime of a movie."
      ],
      "metadata": {
        "id": "-3W0WrQeO2RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict movie revenue using movie runtime\n",
        "X_train_runtime = np.reshape(regression_df.query ('wiki_id in @train_wikiids').runtime.values, (-1, 1))\n",
        "X_test_runtime = np.reshape(regression_df.query ('wiki_id in @test_wikiids').runtime.values, (-1, 1))\n",
        "lr_m, _, err_m = train_and_test (X_train_runtime, y_train, X_test_runtime, y_test)\n",
        "print (f\"RMSE: {err_m:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2jz6nbFY9kf",
        "outputId": "d08b3854-5d74-4cc4-d95b-3ceb0fdad2de"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 128530237.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!** Fill all the cells below to get the RMSE for individual models [0.5 points]"
      ],
      "metadata": {
        "id": "qLFZM5trg57M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the movie revenue using the genres of release.\n",
        "# This is where you should call make_genre_features and train_test_indices\n",
        "\n",
        "X_train_genres = None\n",
        "X_test_genres = None\n",
        "\n",
        "# Your code below to calculate the feature matrices\n",
        "\n",
        "# Train and evaluate\n",
        "lr_g, _, err_g = train_and_test (X_train_genres.toarray(), y_train, X_test_genres.toarray(), y_test)\n",
        "print (f\"RMSE: {err_g:.2f}\")"
      ],
      "metadata": {
        "id": "h-Iv7aKH-amX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the movie revenue using the tfidf vectors of text in plot summaries\n",
        "\n",
        "X_train_summary = None\n",
        "X_test_summary = None\n",
        "\n",
        "# Your code below to calculate the feature matrices\n",
        "\n",
        "# Train and evaluate\n",
        "lr_s, _, err_s = train_and_test (X_train_summary.toarray(), y_train, X_test_summary.toarray(), y_test)\n",
        "print (f\"RMSE: {err_s:.2f}\")"
      ],
      "metadata": {
        "id": "uCik9lOyjQvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the movie revenue using the topic vectors for each movie\n",
        "\n",
        "X_train_topics = None\n",
        "X_test_topics = None\n",
        "\n",
        "# Your code below to calculate the feature matrices\n",
        "\n",
        "# Train and evaluate\n",
        "lr_t, _, err_t = train_and_test (X_train_topics, y_train, X_test_topics, y_test)\n",
        "print (f\"RMSE: {err_t:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snw-XCfrkmKS",
        "outputId": "01ed4f55-22a6-4881-e691-62a744aa920b"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 119915377.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the movie revenue using movie length and the genre features\n",
        "# Note: When creating a combined feature matrix, you may want to convert the\n",
        "# individual feature matrices as numpy arrays\n",
        "X_train_runtime_genres = None\n",
        "X_test_runtime_genres = None\n",
        "\n",
        "# Your code below to calculate the feature matrices\n",
        "\n",
        "# Train and evaluate\n",
        "lr_m_g, _, err_m_g = train_and_test (X_train_runtime_genres,\n",
        "                                     y_train,\n",
        "                                     X_test_runtime_genres,\n",
        "                                     y_test)\n",
        "print (f\"RMSE: {err_m_g:.2f}\")"
      ],
      "metadata": {
        "id": "HRxvL0_umanj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the movie revenue using movie length and the decade features\n",
        "# Note: When creating a combined feature matrix, you may want to convert the\n",
        "# individual feature matrices as numpy arrays\n",
        "import scipy.sparse\n",
        "X_train_genres_summary = None\n",
        "X_test_genres_summary = None\n",
        "\n",
        "# Your code below to calculate the feature matrices\n",
        "\n",
        "# Train and evaluate\n",
        "lr_g_s, _, err_g_s = train_and_test (scipy.sparse.csr_matrix (X_train_genres_summary),\n",
        "                                     y_train,\n",
        "                                     scipy.sparse.csr_matrix (X_test_genres_summary),\n",
        "                                     y_test)\n",
        "print (f\"RMSE: {err_g_s:.2f}\")"
      ],
      "metadata": {
        "id": "VX4KrLxUok_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the movie revenue using movie length and the decade features\n",
        "# Note: When creating a combined feature matrix, you may want to convert the\n",
        "# individual feature matrices as numpy arrays\n",
        "import scipy.sparse\n",
        "X_train_summary_topics = None\n",
        "X_test_summary_topics = None\n",
        "\n",
        "# Your code below to calculate the feature matrices\n",
        "\n",
        "# Train and evaluate\n",
        "lr_s_t, _, err_s_t = train_and_test (scipy.sparse.csr_matrix (X_train_summary_topics),\n",
        "                                     y_train,\n",
        "                                     scipy.sparse.csr_matrix (X_test_summary_topics),\n",
        "                                     y_test)\n",
        "print (f\"RMSE: {err_s_t:.2f}\")"
      ],
      "metadata": {
        "id": "g6nGGZrVqKj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the movie revenue using movie length, genre, and plot summaries\n",
        "# Note: When creating a combined feature matrix, you may want to convert the\n",
        "# individual feature matrices as numpy arrays\n",
        "import scipy.sparse\n",
        "X_train_runtime_genres_summary = None\n",
        "X_test_runtime_genres_summary = None\n",
        "\n",
        "# Your code below to calculate the feature matrices\n",
        "\n",
        "# Train and evaluate\n",
        "lr_m_g_s, _, err_m_g_s = train_and_test (scipy.sparse.csr_matrix (X_train_runtime_genres_summary),\n",
        "                                     y_train,\n",
        "                                     scipy.sparse.csr_matrix (X_test_runtime_genres_summary),\n",
        "                                     y_test)\n",
        "print (f\"RMSE: {err_m_g_s:.2f}\")"
      ],
      "metadata": {
        "id": "1A7cMo37rJtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the movie revenue using genre, plot summaries, and topics\n",
        "# Note: When creating a combined feature matrix, you may want to convert the\n",
        "# individual feature matrices as numpy arrays\n",
        "import scipy.sparse\n",
        "X_train_genres_summary_topics = None\n",
        "X_test_genres_summary_topics = None\n",
        "\n",
        "# Your code below\n",
        "\n",
        "# Train and evaluate\n",
        "lr_g_s_t, _, err_g_s_t = train_and_test (scipy.sparse.csr_matrix (X_train_genres_summary_topics),\n",
        "                                     y_train,\n",
        "                                     scipy.sparse.csr_matrix (X_test_genres_summary_topics),\n",
        "                                     y_test)\n",
        "print (f\"RMSE: {err_g_s_t:.2f}\")"
      ],
      "metadata": {
        "id": "ueQMGsIICWYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the movie revenue using all the features\n",
        "# Note: When creating a combined feature matrix, you may want to convert the\n",
        "# individual feature matrices as numpy arrays\n",
        "import scipy.sparse\n",
        "X_train_all = None\n",
        "X_test_all = None\n",
        "\n",
        "# Your code below\n",
        "\n",
        "# Train and evaluate\n",
        "lr_all, _, err_all= train_and_test (scipy.sparse.csr_matrix (X_train_all),\n",
        "                                     y_train,\n",
        "                                     scipy.sparse.csr_matrix (X_test_all),\n",
        "                                     y_test)\n",
        "print (f\"RMSE: {err_all:.2f}\")"
      ],
      "metadata": {
        "id": "wBHBtXY5rKzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra credit [1 point]\n",
        "\n",
        "Try to reduce the RMSE of the best-performing model. Potential ways to reduce RMSE:\n",
        "\n",
        "- Include more topics\n",
        "- Include more genres\n",
        "- Perform non-linear regression\n",
        "\n",
        "Try one of the above or your own idea to reduce RMSE"
      ],
      "metadata": {
        "id": "7v21WdJgEj0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confidence intervals over coefficients [2 points]\n",
        "\n",
        "Our regression model gives us the coefficients associated with the features. In this section, we'll obtain the confidence intervals of these coefficients using bootstrapping. To obtain the confidence intervals around any coefficient, we would need to train the model on different samples obtained by sampling with replacement and calculate the CI empirically."
      ],
      "metadata": {
        "id": "_77PLSvEFWKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!** Write a function to calculate the coefficients of a linear regression model on some training data a given number of times [0.5 points]"
      ],
      "metadata": {
        "id": "t0m18mdxF6I0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bootstrapped_regression (X_train, y_train, num_bootstraps=1000):\n",
        "  \"\"\" Fit linear models on bootstrapped samples\n",
        "\n",
        "  :params:\n",
        "  X_train (np.array): The train feature matrix\n",
        "  y_train (np.array): The train output values\n",
        "  num_bootstraps (int): 100\n",
        "\n",
        "  :returns:\n",
        "  coeffs (list): The coeffcients for all the features for every boostrap sample\n",
        "  \"\"\"\n",
        "\n",
        "  coeffs = list ()\n",
        "  # Your code below\n",
        "\n",
        "  return coeffs"
      ],
      "metadata": {
        "id": "qQhSJFk0Gzu-"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!** Write a function to calculate the empirical confidence interval for a coefficient estimate [0.5 points]"
      ],
      "metadata": {
        "id": "IG35907SXUrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_empirical_CI (values, lower=0.025, upper=.975):\n",
        "  \"\"\" Calculate the empirical confidence interval from the given values\n",
        "\n",
        "  :params:\n",
        "  :values (np.array): Values of bootstrapped estimates\n",
        "  :lower (float): The lower percentile (default:2.5)\n",
        "  :upper (float): The upper percentile (default:97.5)\n",
        "\n",
        "  :returns: pair as a tuple\n",
        "  :lower_bound (float): The lower bound of the CI\n",
        "  :upper_bound (float): The upper bound of the CI\n",
        "\n",
        "  \"\"\"\n",
        "  # Your code below\n",
        "  pass"
      ],
      "metadata": {
        "id": "gAkaxFVlTH7g"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your turn!** Calcualte the CI for coefficient estimates for the following\n",
        "\n",
        "a. runtime [0.5 points]\n",
        "\n",
        "b. genre corresponding to comedy [0.5 points]\n",
        "\n",
        "What can you tell about the statistical significance of the estimate from the confidence interval for both the variables?\n",
        "\n",
        "**Note:** Do 1000 bootstrapped intervals"
      ],
      "metadata": {
        "id": "fAgMRxBNYJEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coeffs = bootstrapped_regression (X_train_runtime, y_train, num_bootstraps=1000)\n",
        "lb, ub = get_empirical_CI (np.array([item[0] for item in coeffs]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaOajNC4Vavq",
        "outputId": "28f42aac-90b9-4f91-dbff-6c99f6769f1a"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 806.03it/s]\n"
          ]
        }
      ]
    }
  ]
}